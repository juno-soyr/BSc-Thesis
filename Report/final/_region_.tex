\message{ !name(VU-CS-BSc-thesis-template.tex)}\documentclass[11pt]{article}
\usepackage{graphicx}

      \textwidth 15cm
      \textheight 22cm
      \parindent 10pt
      \oddsidemargin 0.85cm
      \evensidemargin 0.37cm

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{tikz}

\usepackage[backend=biber]{biblatex}
\addbibresource{VU-CS-BSc-thesis-template.bib}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\begin{document}

\message{ !name(VU-CS-BSc-thesis-template.tex) !offset(-3) }


\thispagestyle{empty}

\begin{center}

Vrije Universiteit Amsterdam

\vspace{1mm}

\includegraphics[height=28mm]{vu-griffioen.pdf}

\vspace{1.5cm}

{\Large Bachelor Thesis}

\vspace*{1.5cm}

\rule{.9\linewidth}{.6pt}\\[0.4cm]
{\huge \bfseries Introduction to the Weak Invariance Thesis in $\lambda$-Calculus\par}
\rule{.9\linewidth}{.6pt}\\[1.5cm]

\vspace*{2mm}

{\Large
\begin{tabular}{l}
{\bf Author:} ~~Haileselassie Gaspar ~~~~ (2762335)
\end{tabular}
}

\vspace*{1.5cm}

\begin{tabular}{ll}
  {\it 1st supervisor:}   & ~~Femke van Raamsdonk \\
  {\it 2nd reader:}       & ~~JÃ¶rg Endrullis
\end{tabular}

\vspace*{2cm}

\textit{A thesis submitted in fulfillment of the requirements for\\ the VU Bachelor of Science degree in Computer Science }

\vspace*{1cm}

\today\\[4cm] % Date

\end{center}

\begin{abstract}
  The idea of computational Invariance is one that is essential to complexity theory, ever since Van Emde Boas proposed it~\cite{machine-models}. While this paper relates to many areas of computability theory, we will focus on the invariance of $\lambda$-calculus. Accatoli and Dal Lago proposed, using a calculus with explicit substitutions called the \textbf{Linear Substitution Calculus}, a strategy that relates polynomially to Turing Machines when only time complexity is concerned. In this paper we will introduce said calculus and strategy, and provide an example of how they relate to the classical untyped $\lambda$-calculus .\end{abstract}
\section{Introduction}
During the start of the 20th century, the idea of computability began to be a main mathematical problem. David Hilbert posed the following question in his set of problems to solve during the 20th century: What does it mean for a function to be \textit{computable}? \\
The first step in answering this question would be to define the concept of \textit{computability}. Intuitively, computability is a property of problems or functions that can be solved by some mechanical process. Formally, the set of computable functions can be defined as every function $f : \mathbb{N}^k \rightarrow \mathbb{N}$ for which there exists an effective procedure to compute $f(x)$, where $\mathbb{N}^k$ represents a tupl. By effective procedure we refer to a deterministic collection of operations that can be performed to a collection of symbolic inputs in order to produce a corresponding symbolic output-- cite theory of recursive functions and effective computability --. \\
 Both Alonzo Church and Alan Turing proved almost simultaneously that there exists a model defining the group of computable functions, the former through general recursive functions and the latter with the conceptual \enquote{Turing} machines.
 Besides the computability of said functions, both models provide a method of studying the effective procedure for any function that is computable.
 \subsection{Mathematical Preliminaries}
 An alphabet $\Sigma$ is a non-empty set of symbols of which a language $L$ is conformed. Conversly, a language $L$ is the set of strings formed by elements of an alphabet $\Sigma$.
 --cite Nayuki Minase (10 May 2011). "Countable sets and Kleene star". Project Nayuki. Retrieved 11 January 2012--.Taking an alphabet $A$ and defining $A^0 = \emptyset , A^1 = A , A^{i + 1} = {mn | m \in V^i, n \in V}$ for every $i > 0$.
 We then can define the \textit{Kleene Star} set operator $A*$ as:
 \begin{equation}
   A^* =  \bigcup_{i \leq 0}A^i
 \end{equation}
 A reduction $R$ in a set $A$ is a subset of the set $A \times A$. That is, a set consisting of ordered pairs $(a_n,a_m)$ so that $a_n,a_m \in A$. Note that $a_n$ and $a_m$ can be equal. It is usually denoted by $R : \rightarrow$.
 The reflexive and transitive closures of reductions are denoted as $\xrightarrow{0}$ and $\xrightarrow{i}$ for some $i > 0$ respectively. It is important to note that reductions do not have symmetric properties. For every $(a,b) \in R$, given that $a \neq b$, there is no ordered pair $(b,a) \in R$. \\
 Given a set $A$ and a reduction $R$ defined in said set, an $R$-redex is an element $a \in A$ so that there exists an ordered pair $(a,a') \in R$. In $\lambda$-calculus and the LSC, redexes can appear as subterms of a larger term. A term with no redexes is said to be in $R$-normal form.
\subsection{Machine models}
While there are many models of computation, for the context of this paper it is only necessary to introduce two: Turing Machines, and the $\lambda$-calculus.

\paragraph{Turing Machines}
In 1936, Alan Turing proposed a model now referred to as a \textit{Turing Machine} in order to analyze the Halting Problem~\cite{on-computable-numbers}. This machine has an unlimited memory in the form of a tape, and a set of symbols already present on the tape. This is the main computational model most used when talking about complexity of algorithms~\cite{computation-theory}.

While the details of its logical implementation are not important in this paper, we will note that the reason they are very useful when analyzing the complexity of algorithms is due to the fact that they posess a very clear and stable \textit{cost model}. That is, for every step in an algorithm implemented in a Turing Machine, there is a constant cost in both terms of space and time.

\paragraph{Lambda calculus}
The lambda calculus was conceived as a foundational system for mathematics and logic in the 1930s by Alonzo Church, although this initial calculus was proved inconsistent by Kleene and Rosser in~\cite{rosser-kleene-inconsistency}. This led to Church publishing in 1936 a simplified version of this system with a focus on computability, now called the \textit{untyped lambda calculus}. A more formal introduction to this system will be provided in Section~\ref{intro-lambda}.

There is an encoding from the natural numbers to both $\lambda$-terms and Turing Machines. Therefore, every function $f : \mathbb{N}^k \rightarrow \mathbb{N}$ can be represented in both models.
\subsection{Invariance}
Due to the design of Turing Machines, they serve as an accurate and measurable way of understanding the space and time that computations need in order to produce an output. While they represent the same functions as $\lambda$-calculus does (computable functions), the use of a single unitary cost model for a computational step in time, with the number of head movements, and in space, with the amount of cells used, provide a clearer view of the complexity of an algorithm, which is why they are the most used method of measuring complexity classes nowadays. \\
Since both models represent the same functions, it follows that there must be a way to represent the procedures in one model in the other and vice-versa. Here is where the idea of invariance comes in.
Invariance of models, in the words of Van Embde Boas, stipulates the following:
\begin{center}
 \textit{\enquote{Reasonable} machines can simulate each other within a polynomially
bounded overhead in time and a constant-factor overhead in space.}
\end{center}
What this statement proposes that not only is there a method in which machine models can simulate each other, but that there exists such a method so that the overhead is polynomial in time and constant in space. In other words, it would only take a polynomial amount of extra time and a constant amount of extra space to compute a function $f : \mathbb{N}^k \rightarrow \mathbb{N}$ in $\lambda$-calculus than in a Turing machine and vice-versa.
This paper will use the \textit{weak} invariance thesis, meaning the space requirements, a long-standing problem with lambda calculus, will be dropped.
This paper will introduce the reader to many terms and concepts necessary to understand complexity theory, but it should be viewed as nothing more than a small introduction to the subject with an interesting example; the invariance of the complete untyped $\lambda$-calculus with regards to Turing machines. \\
This subject has a large body of literature, and we only consider a small fragment of papers. For a more detailed introduction to the topics mentioned, refer to the references.

\section{Theoretical Background}\label{theoretical-background}
  \subsection{Introduction to $\lambda$-calculus}\label{intro-lambda}
The idea of $\lambda$-calculus, or its current use, is based on the idea of computability of functions. When talking about functions there are two main ways to view them. The \textit{extensional} view, which observes only the mapping from input to output, and the \textit{intensional} view, which treats functions not as just a mapping, but a rule. This means that if two functions are given by the same formula, they are \textit{intensionally equal}. This allows mathematicians and computer scientists to talk about the behaviour of a function outside of just what it produces~\cite{selinger}.

The $\lambda$-calculus uses the intensional view of functions to treat them as expressions, and analyze their behaviour and, more importantly, their computability. \\
In order to talk about the invariance of $\lambda$-calculus it is first necesary to define some notation that will be used in this paper.
\paragraph{Terms} Assuming a countably infinite set $\mathcal{VAR} = \{ x, y, z, \cdots \}$ of variables, let $M, N, P....$ denote arbitrary $\lambda$-terms. The set of $\lambda$-terms $\Lambda$ is inductively defined as:
\begin{equation*}
  \begin{split}
  & \text{Variables: } x \in \Lambda \\
  & \text{Abstraction: } M \in \Lambda \implies ( \lambda x.M ) \in \Lambda \\
  & \text{Application: } M, N \in \Lambda \implies (M N) \in \Lambda \\
  \end{split}
\end{equation*}
\begin{equation*}
C ::= \langle \cdot \rangle \ | \ \lambda x.C \ | \ Ct \ | \ tC
\end{equation*}
\paragraph{Substitution} This topic will be expanded on Section~\ref{reduction}, but a basic understanding is provided here.
The result of substituting $N$ for the free ocurrences of $x$ in $M$ (notation $M \{ x:= N \} $) is inductively defined as:
\begin{equation*}
  \begin{split}
    & x\{ x := N \} \equiv N \\
    & y \{ x := N \} \equiv y \text{ iff } x \neq y \\
    & (\lambda y.M_{1}) \{ x:=N \} \equiv \lambda y. ( M_{1} \{ x:=N \} ) \\
    & (M_{1}M_{2}) \{ x:=N \} \equiv (M_{1}\{ x:=N \} )(M_{2} \{ x:=N \} )
  \end{split}
\end{equation*}
A free variable is any variable not bound by an abstraction. For example:
\begin{equation*}
  (\lambda x . x y z )
\end{equation*}
In this term, $y$ and $z$ are not bound, therefore free, while $x$ is bound by the abstraction $\lambda x$.
When replacing terms it is important to take care not to accidentally bind a free variable by mistake. That is why when replacing terms, if any free variables would become bound by the replacement, they must be renamed. This is called $\alpha$-conversion.
Note that we use braces to denote \textit{implicit} substitutions due to the fact that when introducing \textit{explicit} substitutions we will use the classical brackets, but when reading literature of the classical lambda-calculus, square brackets will mean implicit substitution.
For further reading on the syntax and axioms of the lambda calculus, refer to~\cite{barendregt1984lambda}.

\subsubsection{Conversion}
This section will be introduced non-mathematically, since an intuitive understanding of $\alpha$-conversion suffices to understand the main points of the proof. However, to understand the more finer points, the reader should again refer to~\cite{barendregt1984lambda}.
When referring to conversion in the $\lambda$-calculus, it is usually in the context of renaming bound variables. As these variables are already \enquote{locked} in regards to an abstraction, any renaming of these with the abstraction to another variable name is an idempotent operation, and can be done at any time.
The need for this comes from the unintentional possible binding of a variable during a substitution step. A short example:
\begin{equation}
  \begin{split}
    & \text{Let } F = \lambda xy.yx \text{, then } \forall M, N: \\
    & FMN = NM
  \end{split}
\end{equation}
This would follow from the inductive definition of substitution, however, when taking $M = y$ and $N = x$, substituting these terms into the expression leads to the equality $xy = xx$, an obvious contradiction.
This comes due to the fact that the substitution of $N$ in $M$ should not capture any free variables in $N$.
\subsubsection{$\beta$-Reduction}\label{reduction}
$\beta$-reduction is a relation $\Lambda \times \Lambda$ based on the substitution rule introduced earlier:
\begin{equation}
  \beta : ( \lambda x.M ) N \rightarrow M [ x:=N ]
\end{equation}
A \textit{$\beta$-redex} is a term or subterm that is of the form $(\lambda x . M)N$.
Now when talking about introducing a \enquote{computational cost}, $\beta$-reduction is the reasonable choice, as it seems to provide a relation to transitions in a Turing machine. However, the problem in this case is with the arbitrary duplication of terms that can occur during a reduction.
But before diving into that, a brief explanation of reduction strategies.
\paragraph{Reduction Strategies}
When talking about reduction strategies, we refer to a map $F : \Lambda \rightarrow \Lambda$ such that for all terms $M \in \Lambda : M \xrightarrow{*} F(M)$. That is, there exists one and only one term to which $M$ reduces in this map. Strategies are defined as terminating if for every $M$ with a normal form, for some $n \in \mathbb{N}$,$ F^{n} (M)$ is in normal form, where $M \xrightarrow{n} F^n(M)$. \\
The reduction strategy that~\cite{beta-invariance} uses is named Leftmost Outermost. This is a normalizing strategy where if $M$ is not in normal form, the leftmost-outermost redex is reduced until it is. Formally, we can define this in the syntax of contexts inductively, but first we introduce the notion of ordering on terms. As ppreviously stated, TRS have a notion of ordering that is based on strings, but for a better understanding of it, it could also be seen as a tree. The ordering introduced by Accatoli and Dal Lago in their paper is based on the idea of contexts, generalizing the notion and allowing for relative positioning of subterms without needing to understand or define the whole term. It is defined as follows~\cite{beta-invariance}:
\begin{enumerate}
  \item Outside-in order:
        \begin{enumerate}
          \item Root: $\langle \cdot \rangle \prec_{O} C$ for every context $C \neq \langle \cdot \rangle$.
          \item Contextual closure: If $C \prec_{O} D$ then $E\langle C \rangle \prec_{O} E\langle D \rangle $ for any context $E$.
        \end{enumerate}
  \item Left-to-right order:
        \begin{enumerate}
          \item Application: If $C \prec_{p} t$ and $D \prec_{p} u$ then $Cu \prec_{L} tD$.
          \item Contextual Closure: If $C \prec_{L} D$ then  $E\langle C \rangle \prec_{L} E\langle D\rangle$ for any context $E$.
        \end{enumerate}
\end{enumerate}
Both of these orderings are partial orders in relation to terms. A total order is achieved by joining the two:
\begin{center}
  If $C \prec_{O} D$ and $C \prec_{L} D$ then $C \prec_{LO} D$.
\end{center}
\subsection{Size-exploding family}
As mentioned in the introduction, there is a family of terms in $\lambda$ that, in a linear number of steps, reduces to a term of exponential size. While the invariance result of Accatoli and Dal Lago does not concern itself with space invariance, it is important to note that due to the design of Turing Machines, they have a conceptually different relation between space and time complexity as $\lambda$ does. While the former has the space complexity be bound by the time complexity, that is, the number of used cells on the tape must be equal or less to the amount of times the head moves, the latter has the opposite. In $\lambda$-calculus, time complexity, or the amount of $\beta$-reduction steps, is bound by the size of the inital and final size. That means that to provide an accurate invariance result, we not only need to fix the inconsistencies with the number of reduction steps in $\lambda$, but we must also take care to show the normal form of a term in a way that is at least polynomially related to the size of the initial term, and the number of steps taken, closing the gap between the two ideas.
\\ \\
Consider the example provided by Accatoli and Dal Lago in~\cite{beta-invariance}. It is introduced here to make the following sections easier to follow. We will also define the ordering of the subterms, and show that $LO$ is a total order on it.
\begin{center}
  Let $u = yxx$, and consider the sequence of terms $t_{n} \in \Lambda_{Exp}$  \\ for $n \in \mathbb{N}$ be defined inductively as \\
  $t_{0} = u \ , \ t_{n + 1} = (\lambda x.t_{n})u$ for every $n \in \mathbb{N}$.
\end{center}

\begin{lemma}\label{lemma1}
  Given a tern of the form $(\lambda x .t_n)a$ for some $a \in \Lambda$:
  \begin{equation}
    (\lambda x .t_n)a \xrightarrow[LO \beta]{1} (\lambda x.t_{n-1})yaa
  \end{equation}
\end{lemma}
\begin{proof}
  \begin{equation}
    (\lambda x . t_n)a = (\lambda x .(\lambda x .t_{n-1})u)a \xrightarrow[LO \beta]{1} (\lambda x . t_{n-1})yxx \{ x \leftarrow a \} = (\lambda x .t_{n-1})yaa
  \end{equation}
\end{proof}
\begin{lemma}
  Given a term of the form $t_{n+1} = (\lambda x. t_n)u$:
  \begin{enumerate}
    \item Its $\beta$-normal form will be $r_n = yr_{n}r_{n}$
    \item It will reach $\beta$-normal form in exactly $n+1$ steps.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
    \item By induction:
          \begin{equation}
            \begin{split}
              & t_1 = (\lambda x .t_0)u \xrightarrow[LO\beta]{1} yuu = yr_0r_0 = r_1 \\
              & t_{n+1} = (\lambda x .t_n) u = (\lambda x. (\lambda x . t_{n-1})u)u \xrightarrow[LO\beta]{1} (\lambda x .t_{n_1})(yuu) = \\
              & (\lambda x. t_{n -1})r1 \xrightarrow[LO\beta]{*}r_{n+1} = y(r_{n}r_{n})
            \end{split}
          \end{equation}
          By Lemma 2.1, any term of the form $(\lambda x. t_n)a$ reduces in one step to $(\lambda  x .t_{n-1})yaa$,and so any term $(\lambda x.t_n)u$ will evaluate to $(\lambda x .t_{n-1})r_1$, therefore, it is clear that the normal form of $t_{n+1}$ is $r_{n+1} = y(r_n r_n)$
    \item By contradiction:
          Assume that the number of steps from $t_n$ to $r_n$,$k$ for $\rho  : t_n \xrightarrow[LO\beta]{k} r_n$, is $n-1$. Then, after $n-1$ LO $\beta$ steps, we reach $r_n$, but by Lemma 2.2.1, and Lemma 2.1 we can see that the form of $t_n$ after $n-1$ steps will be $(\lambda x . u)r_{n-1} \neq r_n$
          Now assume that $k = n+1$. Then after $n$ steps there should be another LO $\beta$ step, from $r_n$, but by definition of the normal form, this is impossible.

  \end{enumerate}
\end{proof}
\begin{theorem}
Every $\lambda$-term $t \in \Lambda_{Exp}$ reduces in $n$ LO $\beta$-reduction steps to a term $r_n$, with $|r_n| \in O(2^n)$.
\end{theorem}
\begin{proof}
It follows from Lemma 2.1 and 2.2.
\end{proof}

As an example for the rest of the paper, we will use the term $t_{2}$ from this family. In order to define the ordering of this term, it must be first converted to a context based syntax:
\[ C_{0} \langle \lambda x. C_{1} \langle \lambda x. C_{3} \langle C_{4} \langle yxx \rangle C_{5} \langle yxx \rangle \rangle C_{2} \langle yxx \rangle \rangle \]
An ordering on this term in $LO$ would then be: -- fix this --
\begin{equation}
 C_{0} \prec_{LO} C_{1} \prec_{LO} C_{2} \prec_{LO} C_{3} \prec_{LO} C_{4} \prec_{LO} C_{5}
\end{equation}
by means of the contextual closure of $LO$.
For further reading on the syntax and axioms of the lambda calculus, refer to~\cite{barendregt1984lambda}.
\subsection{LSC}
In order to properly illustrate the properties of the LSC, we will use as an example the \textbf{size exploding} family that was introduced in~\ref{intro-lambda}. Any member of this family could be a good example, but for brevity, this paper will use the previously introduced $t_{3} \equiv \lambda x.((\lambda x. (yxx))(yxx)(yxx))(yxx)$. As explained before, this term, under regular $\beta$-reduction, will evaluate to an exponential length in a number of steps linear to its size, the main obstacle in the invariance of $\lambda$.
The linear substitution calculus is based in \textbf{explicit substitutions}, as oposed to the regular implicit substitutions that $beta$-reduction produces. The grammar is defined as follows:
\begin{equation}
  t, u, r, p = x \ | \ \lambda x.t \ | \ tu \ | \ t[x \leftarrow u]
\end{equation}
where $t[x \leftarrow u]$ is an explicit substitution.
We need to introduce the idea of a new kind of contexts in order to properly define the reduction strategies for the proof of invariance. Besides the addition of explicit substitution to regular contexts, we introduce \textbf{shallow} and \textbf{substitution} contexts ($S$ and $L$ respectively):
\begin{equation}
  \begin{split}
    &S = \langle \cdot \rangle \ | \ \lambda x.S \ | \ St \ | \ tS \ | S[x \leftarrow t] \\
    &L = \langle \cdot \rangle \ | L[x \leftarrow t]
  \end{split}
\end{equation}
The unfolding operation $t\downarrow$ is defined as:
\begin{equation}
  \begin{split}
    &t\downarrow = t \\
    &(\lambda x .t)\downarrow = \lambda x.t\downarrow \\
    &(tu)\downarrow = t\downarrow u \downarrow \\
    &t[x \leftarrow u]\downarrow = t\downarrow \{x \leftarrow u \downarrow \}
  \end{split}
\end{equation}
and the contextual unfolding $t\downarrow_S$ as:
\begin{equation}
  \begin{split}
    &t\downarrow_{\langle \cdot \rangle} = t\downarrow \\
    &t\downarrow_{\lambda x.S} = t\downarrow_S \\
    &t\downarrow_{Su} = t\downarrow_{uS} = t\downarrow_S \\
    &t\downarrow_{S[x \leftarrow u]} = t\downarrow_S \{x \leftarrow u \downarrow \}
  \end{split}
\end{equation}

We introduce here the operational semantics of the LSC.There are two elementary reductions, coupled with $\alpha$-conversion, required to provide the explicit subsitutions up to an equivalnece with the untyped $\lambda$-calculus.
\begin{equation}
  \begin{split}
    &L \langle \lambda x .t \rangle u \rightarrow_{dB} L \langle t [x \leftarrow u] \rangle \\
    &S \langle x \rangle [x \leftarrow u] \rightarrow_{ls} S \langle u \rangle [x \leftarrow u]
  \end{split}
\end{equation}
Based on this,~\cite{beta-invariance} introduces a new reduction strategy, called \textbf{Leftmost-Outermost Useful} or \textbf{LOU} which we will use to show that the size explosion family does indeed not grow exponentially in the LSC under this strategy. It is important to note that while the Size-exploding family does not grow exponentially under the Linear Head Reduction of the LSC~\cite{invariance-of-cost-model}, it would never reach normal form if we included an application before the initial term, and so, an example of the use of LOU on it will show that even when completely reducting the term, the lenght increase will be polynomially related to the size of the initial term.
\subsection{Leftmost-Outermost Useful}\label{lou}
The definition of \textit{usefulness} in ~\cite{beta-invariance} of a substitution step is defined in base to wether a redex is \textit{useful} in its unfolding of the surrounding (shallow) context.
\paragraph{Useful Step} A reduction step is useful if it is either a $dB$-step or a $ls$-step \\ $S\langle x \rangle \rightarrow_{ls} S\langle r \rangle$ so that the unfolding $r \downarrow_S$:
\begin{enumerate}
  \item Contains a $\beta$-redex,
  \item Or is an abstraction and S is an applicative context, that is, a context of the form $A ::= S\langle Lt \rangle$.
\end{enumerate}
We now extend our definition of the complete LO order on redexes to the LSC by introducing a new rule:
\begin{center}
  Substitution: If $C \prec_p t$ and $D \prec_p$ then $C[x \leftarrow u] \prec_L t[x \leftarrow D]$.
\end{center}
Therefore, we can define the \textbf{Leftmost Outermost Useful redex}:
\begin{center}
  A redex $R$ of a term $t$ is considered the leftmost outermost useful redex of $t$ if $R \prec_{LO} Q$ for every other useful redex $Q$ of $t$. We write $t \rightarrow_{LOU} u$ for the step reducing the leftmost outermost useful redex of t.
\end{center}
\section{Generalizing Invariance}
As stated before the measure employed to analyze the time invariance of lambda calculus is the number of transitions in a turing machine. By means of the Linear Substitution Calculus, it is possible to represent even size-exploding terms in Turing machines in polynomial time. The LOU strategy is one strategy that has the properties required for invariance, but that does not mean that it is the only one. Accatoli and Dal Lago in~\cite{beta-invariance} showed that any strategy having the following properties could be an adequate candidate for time invariance.
\subsection{High level implementation systems}
The purpose of the high level implementation system definition is to provide a rewriting system invariant to lambda calculus. This step is a bridge of sorts in between lambda calculus and turing machines. For this, we need to define this class of rewriting systems, and which properties should they satisfy in order to be invariant to lambda calculus. We want spcifically termintation and polynomial overhead.
\[\rightsquigarrow \text{terminates iff} {\rightsquigarrow}_{X} \text{terminates}\]
Furthermore,
\[t {\rightsquigarrow}_{X}^k u \text{ iff } t {\rightsquigarrow}^h u\downarrow \text{with } O(h) \in O(k^n) \text{ for some } n \in \mathbb{N} \text{ and } n \leq 0 \]
We will demonstrate that the LOU reduction strategy follows this generalization with the previous example of the size-exploding family.
\section{Size exploding family}
In these examples we will forego the definition of the usual LO order on contexts and will focus solely on usefulness for the sake of convenience.
Using the term $t_2$ as a first example, the first step is to split the term into (shallow) contexts, and order them according to the LO total order. In this term there is only one outermost context and it is a $dB$ redex, which is inherently useful. Therefore, the first step will be:
\begin{equation*}
  (\lambda x . (\lambda x . yxx) (yxx))(yxx) \rightarrow_{dB} (\lambda x . yxx)(yxx)[x \leftarrow yx'x']
\end{equation*}
Note the $\alpha$-conversion to avoid unwanted variable capture.
We can then perform another $dB$ step, arriving to the term
\begin{equation*}
  (yxx)[x \leftarrow yx'x'][x' \leftarrow yx''x'']
\end{equation*}
At this point, the polynomial algorithm introduced in~\cite{beta-invariance} to find useful $ls$ steps is necessary, although in this paper we will compute them by hand. There is obviously no possible $\beta$-redex of the unfolding of this term, and since there is no abstraction, we can safely say that this term is it $dB$-normal form and $ls$-normal form.

It is important to note that due to this example only requiring head substitution, the Linear Head Reduction introduced in~\cite{invariance-of-cost-model} suffices, but the LOU strategy is necessary for reaching normal form in every case.

\section{Conclusion}
The LSC and the LOU strategy in it provide a basis for future improvements in the study of computational invariance between the $\lambda$-calculus and Turing Machines. This could lead to represent polynomial classes such as P, NP and PSPACE in $\lambda$-calculus, and investigating their properties from a different viewpoint.
\printbibliography{}


\end{document}

\message{ !name(VU-CS-BSc-thesis-template.tex) !offset(-336) }
