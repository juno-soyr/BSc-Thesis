\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}

\usepackage[backend=biber]{biblatex}
\addbibresource{main.bib}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\title{Cost Models in the Lambda Calculus, and its impact in the Invariance thesis}
\author{Haileselassie Gaspar\\[1cm]{\small Supervisors: Femke V. Raamsdonk, JÃ¶rg Endrullis}}

\begin{document}
\maketitle

\section{Abstract}\label{abstract}
The idea of a cost model for the $lambda$-calculus has
\section{Introduction}
During the start of the 20th century, the idea of computability started to be a main mathematical problem. David Hilbert posed the question in his set of problems to solve during the 20th century: What does it mean for a function to be \textit{computable}? \\
The first step in answering this question would be to define the concept of \textit{computability}. Both Alonzo Church and Alan Turing proved almost simultaneously that there existed a model defining the group of computable functions, the former through general recursive functions and the latter with the conceptual 'Turing' machines.
In order to analyze the computability and complexity of functions and algorithms is done through a computational model, or \textit{machine model}.
\subsection{Machine models}
While there are many models of computation, for the context of this paper it is only necesary to introduce two.

\paragraph{Turing Machines}
In 1936, Alan Turing proposed a model now referred as a \textit{Turing Machine} in order to analyze the halting problem~\cite{on-computable-numbers}. This machine has an unlimited memory in the form of a tape, and a set of symbols already present on the tape. It uses a head to read and write symbols from the tape, and a transition function determines where the head moves after a read, either left or right. This is the main computational model most used when talking about complexity of algorithms~\cite{computation-theory}.

\paragraph{Lambda calculus}
The lambda calculus was conceived as a foundational system for mathematics and logic in the 1930s by Alonzo Church. Although this initial idea was proved inconsistent by Kleene and Rosser in~\cite{rosser-kleene-inconsistency}. This led to Church publishing in 1936 a simplified version of this system with a focus on computability, now called the \textit{untyped lambda calculus}. A more formal introduction to this system will be provided in section\ref{intro-lambda}. $lambda$-calculus can also be seen as a Term Rewriting System, another concept that will be introduced in detail in section\ref{intro-trs}.
\subsection{Equivalence and Invariance}
-- equivalence part ; turing equivalence, turing completeness ;
When we talk about invariance we refer to it in the sense of Van Embde Boas~\cite{machine-models}: \\
\begin{center}
 \textit{\enquote{Reasonable} machines can simulate each other within a polynomially
bounded overhead in time and a constant-factor overhead in space.}
\end{center}

This paper will use the \textit{weak} invariance thesis, meaning the space requirements, a long-standing problem with lambda calculus, will be dropped.
\subsection{Introduction to Term Rewriting}
Since this paper is based in great part in the $lambda$-calculus, it is necesary to introduce the idea of $lambda$-calculus not only as a specific set of terms and equations in an unfamiliar form to anyone without a background in theoretical computer science, but as a member of a greater group of systems that rely on the idea of reduction as a relation on terms. This group is referred as Term Rewriting Systems, and it originated on predicate logic, and was first formally defined by Axel Thue in a paper in 1910.\\
It builds on the notion of \enquote{directed} computation, in the sense that the relations defined in such systems ar unidirectional unless otherwise specified. These systems can therefore represent programs by using the notion of reductions as a correspondance to the idea of program evaluation. An interesting result, is the fact that Term Rewriting Systems (TRS for short) are Turing Complete, and therefore, seeing as $lambda$-calculus is a type of TRS, it follows that it is also Turing Complete. It was later proven by Rosser and Kleene that it is also Turing Equivalent, and therefore, the idea of Invariance is not out of the realm of the impossible. Since both models define the same group of functions, would it not follow that there is an algorithm that can \enquote{reasonably} simulate each of them?
\section{Theoretical Background}\label{theoretical-background}
\subsection{Term Rewriting Systems}\label{intro-trs}
While term rewriting knowledge is not necessary to understand and use the $lambda$-calculus, in order to talk more in depth about its qualities and properties, it will be helpful to have a small introduction into the topic. \\
To talk about the idea of Term Rewriting Systems,
\subsection{Introduction to lambda calculus}\label{intro-lambda}
The idea of $lambda$-calculus, or its current use, is based on the idea of computability of functions. When talking about functions there are two main ways to view them. The \textit{extensional} view, which observes only the mapping from input to output, and the \textit{intensional} view, which treats functions not as just a mapping, but a rule. This means that if two functions are given by the same formula, they are \textit{intensionally equal}. This allows mathematicians and computer scientists to talk about the behaviour of a function outside of just what it produces~\cite{selinger}.

The $lambda$-calculus uses the intensional view of functions to treat them as expressions, and analyze their behaviour and, more importantly, their computability. It is important to notice that since Turing machines and the $lambda$-calculus treat functions \textit{intensionally}, most of the proof is centered around the process of computation.
In order to talk about the invariance of $\lambda$-calculus it is first necesary to define some notation that will be used in this paper.
\paragraph{Terms} Let $M, N, P....$ denote arbitrary $\lambda$-terms, and $x,y,z...$ denote variables. The set of $\lambda$-terms $\Lambda$ is inductively defined as:
\begin{equation}
  \begin{split}
  & \text{Variables: } x \in \Lambda \\
  & \text{Abstraction: } M \in \Lambda \implies ( \lambda x.M ) \in \Lambda \\
  & \text{Application: } M, a
  \end{split}
\end{equation}

\paragraph{Free Variables} $FV(M)$ is the set of free variables in $M$ and it includes every variable in $M$ not bound by an abstraction.
A context is a lambda term with a hole in it that can be replaced by another term or a context. They are defined as:
\begin{equation}
C ::= \langle \cdot \rangle \ | \ \lambda x.C \ | \ Ct \ | \ tC
\end{equation}
\paragraph{Substitution} This topic will be expanded on section \ref{reduction}, but a basic understanding is provided here.
The result of substituting $N$ for the free ocurrences of $x$ in $M$ (notation $M[x:= N]$) is inductively defined as:
\begin{equation}
  \begin{split}
    & x [x := N] \equiv N \\
    & y [ x=N ] \equiv y \text{ iff } x \neq y \\
    & (\lambda y.M_{1})[ x:=N ] \equiv \lambda y. ( M_{1} [ x:=N ] ) \\
    & (M_{1}M_{2})[x:=N] \equiv (M_{1}[x:=N])(M_{2}[x:=N])
  \end{split}
\end{equation}
For further reading on the syntax and axioms of the lambda calculus, refer to ~\cite{barendregt1984lambda}.

\subsubsection{Conversion}
This section will be introduced non-mathematically, since an intuitive understanding of $alpha$ conversion suffices to understand the main points of the proof. However, to understand the more finer points, the reader should again refer to ~\cite{barendregt1984lambda}.
When referring to conversion in the $lambda$-calculus, it is usually in the context of renaming bound variables. As these variables are already 'locked' in regards to an abstraction, any renaming of these with the abstraction to another variable name is an idempotent operation, and can be done at any time.
The need for this comes from the unintentional possible binding of a variable during a substitution step. A short example:
\begin{equation}
  \begin{split}
    & \text{Let } F = \lambda xy.yx \text{, then } \forall M, N: \\
    & FMN \equiv NM
  \end{split}
\end{equation}
This would follow from the inductive definition of substitution, however, when taking $M = y$ and $N = x$, this leads to the expression $xy \equiv xx$.
This comes due to the fact that the substitution of $N$ in $M$ should not capture any free variables in $N$.
\subsubsection{Reduction}
\label{reduction}
A reduction in $lambda$-calculus can be defined as a conversion between terms that contracts the term tree. In this sense, it can be seen as a simplification of the multiple abstractions and applications in a term to a more simple, albeit long, form. Once a term is at a point were it has no possible reduction possible, it is in what is called \enquote{normal form}.
\paragraph{Reduction} Let $\textbf{R}$ be a notion of reduction on $\Lambda$. Then $\textbf{R}$ induces the binary relations:
\begin{equation}
  \begin{split}
          &{\rightarrow}_{R} \ \textit{one step R-reduction} \\
          &\rightarrow_{R}^{*} \textit{R-reduction} \\
          &=_{R} \ \textit{R-equality or R-convertibility}
  \end{split}
\end{equation}
On this simple idea of reduction we can define the classical notion of reduction in the lambda calculus, $\beta$-reduction. When talking about measuring time complexity in the $lambda$-calculus, this is a good place to start, as it is the main computational device used.
It is based on the substitution rule introduced earlier:
\begin{equation}
  \beta : ( \lambda x.M ) N \rightarrow M [ x:=N ]
\end{equation}
A brief introduction to some terms associated with reduction; An \textit{R-redex} is a term or subterm that is not in $R$-normal form.
Now when talking about introducing a \enquote{computational cost},$\beta$-reduction is the reasonable choice, as it seems to provide a relation to transitions in a Turing machine. However, the problem in this case is with the arbitrary duplication of terms that can occur during a reduction.
But before diving into that, a brief explanation of reduction strategies.
\paragraph{Reduction Strategies}
- LO order on reductions
\subsubsection{Church Rosser And Standarization}
- Diamond property - Is LO CR?
- What does it mean to be standard? - Is LO Standard?
For further reading on the syntax and axioms of the lambda calculus, refer to~\cite{barendregt1984lambda}.
\subsection{LSC}
- Pretty obvious
\section{Proof Overview}
As stated before the measure employed to analyze the time invariance of lambda calculus is the number of transitions in a turing machine. By means of the Linear Substitution Calculus, it is possible to represent even size-exploding terms in Turing machines in polynomial time. It will be shown that by converting the LSC to a NPDA, the lambda calculus is indeed quadraticly bound in time when representing Turing machines.
\subsection{High level implementation systems}
The purpose of the high level implementation system definition is to provide a rewriting system invariant to lambda calculus. This step is a bridge of sorts in between lambda calculus and turing machines. For this, we need to define this class of rewriting systems, and which properties should they satisfy in order to be invariant to lambda calculus. We want spcifically termintation and polynomial overhead.
$$\rightsquigarrow \text{terminates iff} {\rightsquigarrow}_{X} \text{terminates}$$
Furthermore,
$$t {\rightsquigarrow}_{X}^k u iff t {\rightsquigarrow}^h u\downarrow \text{with } O(h) \in O(k^n) \text{ for some } n \in \mathbb{R} $$
\subsubsection{Properties of high level systems}
This section will provide a basis for the properties of high level systems, and define the properties that they contain.
\subsubsection{Proof of high level properties}
- Termination and polynomial overhead for a generic LSC term
\subsection{Low level implementation}
A high level implementation system is implemented on a turing machine with an overhead in time polynomial to k and the size of the initial term.
\subsubsection{Properties of low level systems}
- Subterm
- Selection
\subsubsection{Proof of low level propeties}
- Polynomial bound on reductions in LSC strategy.
\subsection{Useful derivations}
- What does it mean for a derivation to be useful?
- Why do we need useful derivations?
- Leftmost Outermost Useful
\subsection{Standarization of Useful derivations}
- Why do LSC srategies contain the subterm property?
- Why does LOU have the subterm property?
\section{Comparing LSC terms}
- Look into the algorithm to compare them and talk about it
- If we can define an equality relation in the LSC, then we can prove basically the same as we can in Lambda calculus.

\printbibliography
\end{document}
