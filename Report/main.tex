\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{tikz}

\usepackage[backend=biber]{biblatex}
\addbibresource{main.bib}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\title{Introduction to the (Weak) Invariance Thesis in $\lambda$-Calculus}
\author{Haileselassie Gaspar\\[1cm]{\small Supervisors: Femke van Raamsdonk, JÃ¶rg Endrullis}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\begin{document}
\maketitle
\begin{abstract}
  The idea of computational Invariance is one that is essential to complexity theory, ever since Slot and Van Emde Boas proposed it in 1984-- add reference--. While this thesis relates to many areas of computability theory, in this paper we will focus on the invariance of $\lambda$-calculus. Accatoli and Dal Lago proposed, using a calculus with explicit substitutions called the \textbf{Linear Substitution Calculus}, a strategy that relates polynomially to Turing Machines when only time complexity is concerned. In this paper we will show, using this strategy, a more developed proof on why the computational steps of $\lambda$ are not invariant with respect to the calculus itself, and provide a proof of the invariance of addition and in this calculus.
\end{abstract}
\section{Introduction}
During the start of the 20th century, the idea of computability started to be a main mathematical problem. David Hilbert posed the question in his set of problems to solve during the 20th century: What does it mean for a function to be \textit{computable}? \\
The first step in answering this question would be to define the concept of \textit{computability}. Intuitively, computability is a property of problems or functions that can be solved by some mechanical process. Both Alonzo Church and Alan Turing proved almost simultaneously that there exists a model defining the group of computable functions, the former through general recursive functions and the latter with the conceptual \enquote{Turing} machines.
The analysis of computability and complexity of functions and algorithms is done through a computational model, or \textit{machine model}.
\subsection{Machine models}
While there are many models of computation, for the context of this paper it is only necessary to introduce two.

\paragraph{Turing Machines}
In 1936, Alan Turing proposed a model now referred to as a \textit{Turing Machine} in order to analyze the Halting Problem~\cite{on-computable-numbers}. This machine has an unlimited memory in the form of a tape, and a set of symbols already present on the tape. This is the main computational model most used when talking about complexity of algorithms~\cite{computation-theory}.

While the details of its logical implementation are not important in this paper, we will note that the reason they are very useful when analyzing the complexity of algorithms is due to the fact that they posess a very clear and stable \textit{cost model}. That is, for every step in an algorithm implemented in a Turing Machine, there is a clear cost both when talking in terms of space and time.

This idea of\textit{cost models} comes from the Halting Problem posed by David Hilbert. While the initial problem relates to Peano arithmetical Axioms, it can be seen in the context of Turing Machines as:

\begin{center}
\textit{\enquote{Is there a Turing Machine $\mathcal{M}$ so that given as an input another Turing Machine $\mathcal{M}'$, $\mathcal{M}$ can decide if $\mathcal{M}'$ halts?}}
\end{center}

While Turing proved that this is impossible in~\cite{on-computable-numbers}, a natural extension of this is to not focus on the decidability of a Turing Machine, but on the number of steps that a Turing Machine that we know is decidable takes to reach a halting state.


\paragraph{Lambda calculus}
The lambda calculus was conceived as a foundational system for mathematics and logic in the 1930s by Alonzo Church. Although this initial idea was proved inconsistent by Kleene and Rosser in~\cite{rosser-kleene-inconsistency}. This led to Church publishing in 1936 a simplified version of this system with a focus on computability, now called the \textit{untyped lambda calculus}. A more formal introduction to this system will be provided in Section~\ref{intro-lambda}. $\lambda$-calculus can also be seen as a Higher-Order Term Rewriting System, another concept that will not be introduced in this paper.

\subsection{Invariance}
Due to the design of Turing Machines, they serve as a more accurate and measurable way of understanding the space and time that computations need in order to produce an output. While they compute the same functions as $\lambda$-calculus does, the use of a single unitary cost model for a computational step in time, with the number of head movements, and in space, with the amount of cells used, provide a clearer view of the complexity of an algorithm, which is why they are the most used method of measuring complexity nowadays.
First, it is important to note the difference between equivalence and invariance, two terms very often used in the literature of computation. Equivalence refers to the ability of any model to simulate a Turing machine, while invariance, in the words of Van Embde Boas, stipulates the following:
\begin{center}
 \textit{\enquote{Reasonable} machines can simulate each other within a polynomially
bounded overhead in time and a constant-factor overhead in space.}
\end{center}
This is an extension to the idea of equivalence between computational models. It proposes that not only is there a method in which machine models can simulate each other, but that there exists such a method so that the overhead is polynomial in time and constant in space. In other words, it would only take a polynomial amount of extra time and a constant amount of extra space to evaluate a term in $\lambda$-calculus than in a Turing machine and viceversa. -- talk about cost models a bit  and explain it in terms of that --
This paper will use the \textit{weak} invariance thesis, meaning the space requirements, a long-standing problem with lambda calculus, will be dropped.
In~\cite{invariance-of-cost-model}, it is proven that there is an implementation of Turing Machines in the deterministic $\lambda$-calculus, with only a linear overhead in time when only head reduction is considered. The differences will be explained in Section~\ref{lou} This paper will introduce the reader to many terms and concepts necessary to understand complexity theory, but it should be viewed as nothing more than a small introduction to the subject with an interesting example, the invariance of the complete untyped $\lambda$-calculus with regards to Turing machines. \\
In order to acquire a more in depth understanding of the topic, refer to the references, that although incomplete, may provide a more formal introduction into the subject.


\section{Theoretical Background}\label{theoretical-background}
  \subsection{Introduction to $\lambda$-calculus}\label{intro-lambda}
  While an introduction to TRS is a proper base to start understanding the concepts of $\lambda$, the idea of it predates that of TRS. \\
The idea of $\lambda$-calculus, or its current use, is based on the idea of computability of functions. When talking about functions there are two main ways to view them. The \textit{extensional} view, which observes only the mapping from input to output, and the \textit{intensional} view, which treats functions not as just a mapping, but a rule. This means that if two functions are given by the same formula, they are \textit{intensionally equal}. This allows mathematicians and computer scientists to talk about the behaviour of a function outside of just what it produces~\cite{selinger}.

The $\lambda$-calculus uses the intensional view of functions to treat them as expressions, and analyze their behaviour and, more importantly, their computability. It is important to notice that since Turing machines and the $\lambda$-calculus treat functions \textit{intensionally}, most of the proof is centered around the process of computation.
In order to talk about the invariance of $\lambda$-calculus it is first necesary to define some notation that will be used in this paper.
\paragraph{Terms} Assuming a countably infinite set $\mathcal{VAR} = \{ x, y, z, \cdots \}$ of variables et $M, N, P....$ denote arbitrary $\lambda$-terms. The set of $\lambda$-terms $\Lambda$ is inductively defined as:
\begin{equation*}
  \begin{split}
  & \text{Variables: } x \in \Lambda \\
  & \text{Abstraction: } M \in \Lambda \implies ( \lambda x.M ) \in \Lambda \\
  & \text{Application: } M, N \in \Lambda \implies (M N) \in \Lambda \\
  \end{split}
\end{equation*}
\begin{equation*}
C ::= \langle \cdot \rangle \ | \ \lambda x.C \ | \ Ct \ | \ tC
\end{equation*}
\paragraph{Substitution} This topic will be expanded on Section~\ref{reduction}, but a basic understanding is provided here.
The result of substituting $N$ for the free ocurrences of $x$ in $M$ (notation $M \{ x:= N \} $) is inductively defined as:
\begin{equation*}
  \begin{split}
    & x\{ x := N \} \equiv N \\
    & y \{ x := N \} \equiv y \text{ iff } x \neq y \\
    & (\lambda y.M_{1}) \{ x:=N \} \equiv \lambda y. ( M_{1} \{ x:=N \} ) \\
    & (M_{1}M_{2}) \{ x:=N \} \equiv (M_{1}\{ x:=N \} )(M_{2} \{ x:=N \} )
  \end{split}
\end{equation*}
A free variable is any variable not bound by an abstraction. For example:
\begin{equation*}
  (\lambda x . x y z )
\end{equation*}
In this term, $y$ and $z$ are not bound, therefore free, while $x$ is bound by the abstraction $\lambda x$.
When replacing terms it is important to take care not to accidentally bind a free variable by mistake. That is why when replacing terms, if any free variables would become bound by the replacement, they must be renamed. This is called $\alpha$-conversion.
Note that we use braces to denote \textit{implicit} substitutions due to the fact that when introducing \textit{explicit} substitutions we will use the classical brackets, but when reading literature of the classical lambda-calculus, square brackets will mean implicit substitution.
For further reading on the syntax and axioms of the lambda calculus, refer to~\cite{barendregt1984lambda}.

\subsubsection{Conversion}
This section will be introduced non-mathematically, since an intuitive understanding of $\alpha$ conversion suffices to understand the main points of the proof. However, to understand the more finer points, the reader should again refer to~\cite{barendregt1984lambda}.
When referring to conversion in the $\lambda$-calculus, it is usually in the context of renaming bound variables. As these variables are already \enquote{locked} in regards to an abstraction, any renaming of these with the abstraction to another variable name is an idempotent operation, and can be done at any time.
The need for this comes from the unintentional possible binding of a variable during a substitution step. A short example:
\begin{equation}
  \begin{split}
    & \text{Let } F = \lambda xy.yx \text{, then } \forall M, N: \\
    & FMN = NM
  \end{split}
\end{equation}
This would follow from the inductive definition of substitution, however, when taking $M = y$ and $N = x$, this leads to the expression $xy \equiv xx$.
This comes due to the fact that the substitution of $N$ in $M$ should not capture any free variables in $N$.
\subsubsection{Reduction}\label{reduction}
A reduction in $\lambda$-calculus can be defined as a conversion between terms that contracts the term tree. In this sense, it can be seen as a simplification of the multiple abstractions and applications in a term to a more simple, albeit long, form. Once a term is at a point were it has no possible reduction possible, it is in what is called \enquote{normal form}.
It is simple to see the relation with the concept of reduction in TRS, and most of the terminology introduced in that section can be used in regards to $\lambda$.
\paragraph{Reduction} Let $\textbf{R}$ be a notion of reduction on $\Lambda$. Then $\textbf{R}$ induces the binary relations:
\begin{equation}
  \begin{split}
          &{\rightarrow}_{R} \ \textit{one step R-reduction} \\
          &\rightarrow_{R}^{*} \textit{R-reduction} \\
          &=_{R} \ \textit{R-equality or R-convertibility}
  \end{split}
\end{equation}
Where $\rightarrow_{R}^{*}$ represents 0, 1 or more reduction steps.
On this simple idea of reduction we can define the classical notion of reduction in the lambda calculus, $\beta$-reduction. When talking about measuring time complexity in the $lambda$-calculus, this is a good place to start, as it is the main computational device used.
It is based on the substitution rule introduced earlier:
\begin{equation}
  \beta : ( \lambda x.M ) N \rightarrow M [ x:=N ]
\end{equation}
An introduction to some terms associated with reduction; An \textit{R-redex} is a term or subterm that is of the form $(\lambda x . M)N$.
Now when talking about introducing a \enquote{computational cost},$\beta$-reduction is the reasonable choice, as it seems to provide a relation to transitions in a Turing machine. However, the problem in this case is with the arbitrary duplication of terms that can occur during a reduction.
But before diving into that, a brief explanation of reduction strategies.
\paragraph{Reduction Strategies}
When talking about reduction strategies, we refer to a map $F : \Lambda \rightarrow \Lambda$ such that for all terms $M \in \Lambda : M \xrightarrow{*} F(M)$. That is, there exists one and only one term to which $M$ reduces in this map. Strategies are defined as terminating if for every $M$ with a normal form, for some $n \in \mathbb{N}$,$ F^{n} (M)$ is in normal form, where $M \xrightarrow{n} F^n(M)$. \\
The reduction strategy that~\cite{beta-invariance} uses is named Leftmost Outermost. This is a normalizing strategy where if $M$ is not in normal form, the leftmost-outermost redex is reduced until it is. Formally, we can define this in the syntax of contexts inductively, but first we introduce the notion of ordering on terms. As ppreviously stated, TRS have a notion of ordering that is based on strings, but for a better understanding of it, it could also be seen as a tree. The ordering introduced by Accatoli and Dal Lago in their paper is based on the idea of contexts, generalizing the notion and allowing for relative positioning of subterms without needing to understand or define the whole term. It is defined as follows~\cite{beta-invariance}:
\begin{enumerate}
  \item Outside-in order:
        \begin{enumerate}
          \item Root: $\langle \cdot \rangle \prec_{O} C$ for every context $C \neq \langle \cdot \rangle$.
          \item Contextual closure: If $C \prec_{O} D$ then $E\langle C \rangle \prec_{O} E\langle D \rangle $ for any context $E$.
        \end{enumerate}
  \item Left-to-right order:
        \begin{enumerate}
          \item Application: If $C \prec_{p} t$ and $D \prec_{p} u$ then $Cu \prec_{L} tD$.
          \item Contextual Closure: If $C \prec_{L} D$ then  $E\langle C \rangle \prec_{L} E\langle D\rangle$ for any context $E$.
        \end{enumerate}
\end{enumerate}
Both of these orderings are partial orders in relation to terms. A total order is achieved by joining the two:
\begin{center}
  If $C \prec_{O} D$ and $C \prec_{L} D$ then $C \prec_{LO} D$.
\end{center}
\subsection{Size-exploding family}
As mentioned in the introduction, there is a family of terms in $\lambda$ that, in a linear number of steps, reduces to a term of exponential size. While the invariance result of Accatoli and Dal Lago does not concern itself with space invariance, it is important to note that due to the design of Turing Machines, they have a conceptually different relation between space and time complexity as $\lambda$ does. While the former has the space complexity be bound by the time complexity, that is, the number of used cells on the tape must be equal or less to the amount of times the head moves, the latter has the opposite. In $\lambda$-calculus, time complexity, or the amount of $\beta$-reduction steps, is bound by the size of the inital and final size. That means that to provide an accurate invariance result, we not only need to fix the inconsistencies with the number of reduction steps in $\lambda$, but we must also take care to show the normal form of a term in a way that is at least polynomially related to the size of the initial term, and the number of steps taken, closing the gap between the two ideas.
\\ \\
Consider the example provided by Accatoli and Dal Lago in~\cite{beta-invariance}. It is introduced here to make the following sections easier to follow. We will also define the ordering of the subterms, and show that $LO$ is a total order on it.
\begin{center}
  Let $u = yxx$, and consider the sequence of terms $t_{n} \in \Lambda_{Exp}$  \\ for $n \in \mathbb{N}$ be defined inductively as \\
  $t_{0} = u \ , \ t_{n + 1} = (\lambda x.t_{n})u$ for every $n \in \mathbb{N}$.
\end{center}

\begin{lemma}\label{lemma1}
  Given a tern of the form $(\lambda x .t_n)a$ for some $a \in \Lambda$:
  \begin{equation}
    (\lambda x .t_n)a \xrightarrow[LO \beta]{1} (\lambda x.t_{n-1})yaa
  \end{equation}
\end{lemma}
\begin{proof}
  \begin{equation}
    (\lambda x . t_n)a = (\lambda x .(\lambda x .t_{n-1})u)a \xrightarrow[LO \beta]{1} (\lambda x . t_{n-1})yxx \{ x \leftarrow a \} = (\lambda x .t_{n-1})yaa
  \end{equation}
\end{proof}
\begin{lemma}
  Given a term of the form $t_{n+1} = (\lambda x. t_n)u$:
  \begin{enumerate}
    \item Its $\beta$-normal form will be $r_n = yr_{n}r_{n}$
    \item It will reach $\beta$-normal form in exactly $n+1$ steps.
  \end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerate}
    \item By induction:
          \begin{equation}
            \begin{split}
              & t_1 = (\lambda x .t_0)u \xrightarrow[LO\beta]{1} yuu = yr_0r_0 = r_1 \\
              & t_{n+1} = (\lambda x .t_n) u = (\lambda x. (\lambda x . t_{n-1})u)u \xrightarrow[LO\beta]{1} (\lambda x .t_{n_1})(yuu) = \\
              & (\lambda x. t_{n -1})r1 \xrightarrow[LO\beta]{*}r_{n+1} = y(r_{n}r_{n})
            \end{split}
          \end{equation}
          By Lemma 2.1, any term of the form $(\lambda x. t_n)a$ reduces in one step to $(\lambda  x .t_{n-1})yaa$,and so any term $(\lambda x.t_n)u$ will evaluate to $(\lambda x .t_{n-1})r_1$, therefore, it is clear that the normal form of $t_{n+1}$ is $r_{n+1} = y(r_n r_n)$
    \item By contradiction:
          Assume that the number of steps from $t_n$ to $r_n$,$k$ for $\rho  : t_n \xrightarrow[LO\beta]{k} r_n$, is $n-1$. Then, after $n-1$ LO $\beta$ steps, we reach $r_n$, but by Lemma 2.2.1, and Lemma 2.1 we can see that the form of $t_n$ after $n-1$ steps will be $(\lambda x . u)r_{n-1} \neq r_n$
          Now assume that $k = n+1$. Then after $n$ steps there should be another LO $\beta$ step, from $r_n$, but by definition of the normal form, this is impossible.

  \end{enumerate}
\end{proof}
\begin{theorem}
Every $\lambda$-term $t \in \Lambda_{Exp}$ reduces in $n$ LO $\beta$-reduction steps to a term $r_n$, with $|r_n| \in O(2^n)$.
\end{theorem}
\begin{proof}
It follows from Lemma 2.1 and 2.2.
\end{proof}

As an example for the rest of the paper, we will use the term $t_{2}$ from this family. In order to define the ordering of this term, it must be first converted to a context based syntax:
\[ C_{0} \langle \lambda x. C_{1} \langle \lambda x. C_{3} \langle C_{4} \langle yxx \rangle C_{5} \langle yxx \rangle \rangle C_{2} \langle yxx \rangle \rangle \]
An ordering on this term in $LO$ would then be: -- fix this --
\begin{equation}
 C_{0} \prec_{LO} C_{1} \prec_{LO} C_{2} \prec_{LO} C_{3} \prec_{LO} C_{4} \prec_{LO} C_{5}
\end{equation}
by means of the contextual closure of $LO$.
For further reading on the syntax and axioms of the lambda calculus, refer to~\cite{barendregt1984lambda}.
\subsection{LSC}
In order to properly illustrate the properties of the LSC, we will use as an example the \textbf{size exploding} family that was introduced in~\ref{intro-lambda}. Any member of this family could be a good example, but for brevity, this paper will use the previously introduced $t_{3} \equiv \lambda x.((\lambda x. (yxx))(yxx)(yxx))(yxx)$. As explained before, this term, under regular $\beta$-reduction, will evaluate to an exponential length in a number of steps linear to its size, the main obstacle in the invariance of $\lambda$.
The linear substitution calculus is based in \textbf{explicit substitutions}, as oposed to the regular implicit substitutions that $beta$-reduction produces. The grammar is defined as follows:
\begin{equation}
  t, u, r, p = x \ | \ \lambda x.t \ | \ tu \ | \ t[x \leftarrow u]
\end{equation}
where $t[x \leftarrow u]$ is an explicit substitution.
We need to introduce the idea of a new kind of contexts in order to properly define the reduction strategies for the proof of invariance. Besides the addition of explicit substitution to regular contexts, we introduce \textbf{shallow} and \textbf{substitution} contexts ($S$ and $L$ respectively):
\begin{equation}
  \begin{split}
    &S = \langle \cdot \rangle \ | \ \lambda x.S \ | \ St \ | \ tS \ | S[x \leftarrow t] \\
    &L = \langle \cdot \rangle \ | L[x \leftarrow t]
  \end{split}
\end{equation}
We introduce here the operational semantics of the LSC. There are two elementary reductions, coupled with $\alpha$-conversion, required to provide the explicit subsitutions up to an equivalnece with the untyped $\lambda$-calculus.
\begin{equation}
  \begin{split}
    &L \langle \lambda x .t \rangle u \rightarrow_{dB} L \langle t [x \leftarrow u] \rangle \\
    &S \langle x \rangle [x \leftarrow u] \rightarrow_{ls} S \langle u \rangle [x \leftarrow u]
  \end{split}
\end{equation}
Based on this,~\cite{beta-invariance} introduces a new reduction strategy, called \textbf{Leftmost-Outermost Useful} or \textbf{LOU} which we will use to show that the size explosion family does indeed not grow exponentially in the LSC under this strategy. It is important to note that while the Size-exploding family does not grow exponentially under the Linear Head Reduction of the LSC~\cite{invariance-of-cost-model}, it would never reach normal form if we included an application before the initial term, and so, an example of the use of LOU on it will show that even when completely reducting the term, the lenght increase will be polynomially related to the size of the initial term.
\subsection{Leftmost-Outermost Useful}\label{lou}
The definition of \textit{usefulness} in ~\cite{beta-invariance} of a substitution step is defined in base to wether a redex is \textit{useful} in its unfolding of the surrounding (shallow) context.
\paragraph{Useful Step} A reduction step is useful if it is either a $dB$-step or a $ls$-step \\ $S\langle x \rangle \rightarrow_{ls} S\langle r \rangle$ so that the unfolding $r \downarrow_S$:
\begin{enumerate}
  \item Contains a $\beta$-redex,
  \item Or is an abstraction and S is an applicative context, that is, a context of the form $A ::= S\langle Lt \rangle$.
\end{enumerate}
We now extend our definition of the complete LO order on redexes to the LSC by introducing a new rule:
\begin{center}
  Substitution: If $C \prec_p t$ and $D \prec_p$ then $C[x \leftarrow u] \prec_L t[x \leftarrow D]$.
\end{center}
Therefore, we can define the \textbf{Leftmost Outermost Useful redex}:
\begin{center}
  A redex $R$ of a term $t$ is considered the leftmost outermost useful redex of $t$ if $R \prec_{LO} Q$ for every other useful redex $Q$ of $t$. We write $t \rightarrow_{LOU} u$ for the step reducing the leftmost outermost useful redex of t.
\end{center}
\section{Generalizing Invariance}
As stated before the measure employed to analyze the time invariance of lambda calculus is the number of transitions in a turing machine. By means of the Linear Substitution Calculus, it is possible to represent even size-exploding terms in Turing machines in polynomial time. The LOU strategy is one strategy that has the properties required for invariance, but that does not mean that it is the only one. Accatoli and Dal Lago in~\cite{beta-invariance} showed that any strategy having the following properties could be an adequate candidate for time invariance.
\subsection{High level implementation systems}
The purpose of the high level implementation system definition is to provide a rewriting system invariant to lambda calculus. This step is a bridge of sorts in between lambda calculus and turing machines. For this, we need to define this class of rewriting systems, and which properties should they satisfy in order to be invariant to lambda calculus. We want spcifically termintation and polynomial overhead.
\[\rightsquigarrow \text{terminates iff} {\rightsquigarrow}_{X} \text{terminates}\]
Furthermore,
\[t {\rightsquigarrow}_{X}^k u \text{ iff } t {\rightsquigarrow}^h u\downarrow \text{with } O(h) \in O(k^n) \text{ for some } n \in \mathbb{N} \text{ and } n \leq 0 \]
We will demonstrate that the LOU reduction strategy follows this generalization by means of 2 examples.
\section{Examples}
\subsection{Size exploding family}
Using the term $t_2$ as a first example, if applying the LOU strategy, we get the following derivation:
\begin{equation}
\begin{split}
  &(\lambda x . ( \lambda x . (yxx) )(yxx))(yxx) \rightarrow_{LOU}  \\
  &(\lambda x . yxx)(yxx)[x \leftarrow (y'x'x')] \rightarrow_{LOU} \\
  &(yxx)[x \rightarrow (y'x'x')][x' \rightarrow (y''x''x'')]
\end{split}
\end{equation}
Note the $\alpha$-conversion in the second step to avoid unwanted variable capture. We can see that a derivation that under LO $\beta$-reduction produced a term of size $2^n$ produces a term linear in both the size of the initial term and the number of steps taken.
It is important to note that due to this example only requiring head substitution, the Linear Head Reduction introduced in~\cite{invariance-of-cost-model} suffices, but the LOU strategy is necessary for reaching normal form in every case, as will be shown by the next example.
\subsection{Addition in the LSC}
We will not delve into the idea of addition and arithmetic operations in $\lambda$-calculus, for more information, we will refer the reader again to~\cite{barendregt1984lambda}. We will only introduce the addition term, that uses currying in order to add the representations of non-negative integers in the $\lambda$-calculus.
The addition term is defined as $ + = \lambda m n f x .(m f)((n f) x)$. This term, coupled with the church numerals representing the non-negative integers $\ulcorner n \urcorner = \lambda f x . f^n (x)$, where $f^n$ represents $n$ applications of $f$, can be used to compute addition in the $\lambda$-calculus.
While addition does have a constant number of reduction steps under regular $\beta$-reduction, we have established that there is a size-exploding problem, that could also present itself in addition. By applying the Leftmost Outermost Useful strategy, we can show that it also takes a constant number of steps, and that

\begin{theorem} Addition takes constant time using the LOU strategy.
\end{theorem}
\section{Comparing LSC Terms}
\section{Conclusion}
\printbibliography
\end{document}
