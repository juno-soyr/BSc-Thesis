\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage{tikz}

\usepackage[backend=biber]{biblatex}
\addbibresource{main.bib}
\usepackage[a4paper, total={6in, 9in}]{geometry}
\title{An Introduction to the Invariance Thesis in $\lambda$-Calculus through Explicit Substitutions}
\author{Haileselassie Gaspar\\[1cm]{\small Supervisors: Femke V. Raamsdonk, JÃ¶rg Endrullis}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\begin{document}
\maketitle
\begin{abstract}
  -- --
\end{abstract}
\section{Introduction}
During the start of the 20th century, the idea of computability started to be a main mathematical problem. David Hilbert posed the question in his set of problems to solve during the 20th century: What does it mean for a function to be \textit{computable}? \\
The first step in answering this question would be to define the concept of \textit{computability}. Intuitively, computability is a property of problems or functions that can be solved by some mechanical process. Both Alonzo Church and Alan Turing proved almost simultaneously that there existed a model defining the group of computable functions, the former through general recursive functions and the latter with the conceptual \enquote{Turing} machines.
The analysis of computability and complexity of functions and algorithms is done through a computational model, or \textit{machine model}.
\subsection{Machine models}
While there are many models of computation, for the context of this paper it is only necesary to introduce two.

\paragraph{Turing Machines}
In 1936, Alan Turing proposed a model now referred as a \textit{Turing Machine} in order to analyze the halting problem~\cite{on-computable-numbers}. This machine has an unlimited memory in the form of a tape, and a set of symbols already present on the tape. It uses a head to read and write symbols from the tape, and a transition function determines where the head moves after a read, either left or right. This is the main computational model most used when talking about complexity of algorithms~\cite{computation-theory}.

\paragraph{Lambda calculus}
The lambda calculus was conceived as a foundational system for mathematics and logic in the 1930s by Alonzo Church. Although this initial idea was proved inconsistent by Kleene and Rosser in~\cite{rosser-kleene-inconsistency}. This led to Church publishing in 1936 a simplified version of this system with a focus on computability, now called the \textit{untyped lambda calculus}. A more formal introduction to this system will be provided in section\ref{intro-lambda}. $lambda$-calculus can also be seen as a Term Rewriting System, another concept that will be introduced in detail later in this section.

\subsection{Equivalence and Invariance}
The importance of Turing machines and their relation to modern computing has made them a central model in the study of computational complexity, in particular when discussing time and space complexity. This has made them a good model by which to measure these qualities, specially since most computational classes are defined in their context ($P, NP$). Therefore, the equivalence and simulation of other models in TMs is one of the main qualities studied in complexity theory.
First, it is important to note the difference between equivalence and completeness, two terms very often used in the literature of computation. Completness refers to the ability of any model to simulate a Turing machine. For those acquainted with languages, one could define completeness as:
\begin{center}
  for a model $\mathcal{M}$, and a deterministic turing machine $TM$, \\
  $L(\mathcal{M}) \supseteq L(TM)$
\end{center}
Equivalence however, is a much more restrictive set. As its name implies, it is the class of every model that is able to compute exactly what a turing machine can. In the previous notation:
\begin{center}
  $L(\mathcal{M}) = L(TM)$
\end{center}
When we talk about invariance we refer to it in the sense of Van Embde Boas~\cite{machine-models}: \\
\begin{center}
 \textit{\enquote{Reasonable} machines can simulate each other within a polynomially
bounded overhead in time and a constant-factor overhead in space.}
\end{center}
This is an extension to the idea of equivalence between computational models. It proposes that not only is there a method in which machine models can simulate each other, but that there exists such a method so that the overhead is polynomial in time and constant in space. In other words, it would only take a polynomial amount of extra time and a constant amount of extra space to evaluate a term in $lambda$-calculus than in a Turing machine and viceversa.
This paper will use the \textit{weak} invariance thesis, meaning the space requirements, a long-standing problem with lambda calculus, will be dropped.
In~\cite{invariance-of-cost-model}, it is proven that there is an implementation of Turing machines in the deterministic $\lambda$-calculus, with only a linear overhead in time when only head reduction is considered. This paper will introduce the reader to many terms and concepts necessary to understand complexity theory, but it should be viewed as nothing more than a small introduction to the subject with an interesting example, the invariance of the complete untyped $\lambda$-calculus with regards to Turing machines. \\
In order to acquire a more in depth understanding of the topic, refer to the references, that although incomplete, may provide a more formal introduction into the subject.
\subsection{Introduction to Term Rewriting}
Since this paper is based in great part in the $\lambda$-calculus, it is useful to introduce the idea of it not only as a specific set of terms and equations in an unfamiliar form to anyone without a background in theoretical computer science, but as a member of a greater group of systems that rely on the idea of reduction as a relation on terms. This group is referred as Term Rewriting Systems, and it originated on predicate logic, and was first formally defined by Axel Thue in a paper in 1910.\\
It builds on the notion of \enquote{directed} computation, in the sense that the relations defined in such systems ar unidirectional unless otherwise specified. These systems can therefore represent programs by using the notion of reductions as a correspondance to the idea of program evaluation. An interesting result, is the fact that there exists a class of Term Rewriting Systems (TRS for short) that are Turing Complete. It was later proven by Rosser and Kleene that $\lambda$ is also Turing Equivalent, and therefore, the idea of invariance is not out of the realm of the impossible. Since both models define the same group of functions, would it not follow that there is an algorithm that can \enquote{reasonably} simulate each of them? \\
The problem with this, is the fact that the main computational step in $\lambda$, the $\beta$-reduction step, is not a unitary measure of complexity costs. There exists a family of terms and subterms that in time linear to the size of themselves, produce an exponential output. This is solved in~\cite{invariance-of-cost-model} and~\cite{beta-invariance} by the use of sharing. This paper aims to provide a somewhat simpler version of the results of both papers, and demonstrate it through an example, a member of this size exploding family.

\section{Theoretical Background}\label{theoretical-background}
  \subsection{Introduction to $\lambda$-calculus}\label{intro-lambda}
  While an introduction to TRS is a proper base to start understanding the concepts of $\lambda$, the idea of it predates that of TRS. \\
The idea of $\lambda$-calculus, or its current use, is based on the idea of computability of functions. When talking about functions there are two main ways to view them. The \textit{extensional} view, which observes only the mapping from input to output, and the \textit{intensional} view, which treats functions not as just a mapping, but a rule. This means that if two functions are given by the same formula, they are \textit{intensionally equal}. This allows mathematicians and computer scientists to talk about the behaviour of a function outside of just what it produces~\cite{selinger}.

The $\lambda$-calculus uses the intensional view of functions to treat them as expressions, and analyze their behaviour and, more importantly, their computability. It is important to notice that since Turing machines and the $\lambda$-calculus treat functions \textit{intensionally}, most of the proof is centered around the process of computation.
In order to talk about the invariance of $\lambda$-calculus it is first necesary to define some notation that will be used in this paper.
\paragraph{Terms} Let $M, N, P....$ denote arbitrary $\lambda$-terms, and $x,y,z...$ denote variables. The set of $\lambda$-terms $\Lambda$ is inductively defined as:
\begin{equation}
  \begin{split}
  & \text{Variables: } x \in \Lambda \\
  & \text{Abstraction: } M \in \Lambda \implies ( \lambda x.M ) \in \Lambda \\
  & \text{Application: } M, N \in \Lambda \implies (M N) \in \Lambda \\
  \end{split}
\end{equation}
\begin{equation}
C ::= \langle \cdot \rangle \ | \ \lambda x.C \ | \ Ct \ | \ tC
\end{equation}
\paragraph{Substitution} This topic will be expanded on section \ref{reduction}, but a basic understanding is provided here.
The result of substituting $N$ for the free ocurrences of $x$ in $M$ (notation $M \{ x:= N \} $) is inductively defined as:
\begin{equation}
  \begin{split}
    & x\{ x := N \} \equiv N \\
    & y \{ x := N \} \equiv y \text{ iff } x \neq y \\
    & (\lambda y.M_{1}) \{ x:=N \} \equiv \lambda y. ( M_{1} \{ x:=N \} ) \\
    & (M_{1}M_{2}) \{ x:=N \} \equiv (M_{1}\{ x:=N \} )(M_{2} \{ x:=N \} )
  \end{split}
\end{equation}
Note that we use braces to denote \textit{implicit} substitutions due to the fact that when introducing \textit{explicit} substitutions we will use the classical brackets, but when reading literature of the classical lambda-calculus, square brackets will mean implicit substitution.
For further reading on the syntax and axioms of the lambda calculus, refer to~\cite{barendregt1984lambda}.

\subsubsection{Conversion}
This section will be introduced non-mathematically, since an intuitive understanding of $\alpha$ conversion suffices to understand the main points of the proof. However, to understand the more finer points, the reader should again refer to~\cite{barendregt1984lambda}.
When referring to conversion in the $\lambda$-calculus, it is usually in the context of renaming bound variables. As these variables are already \enquote{locked} in regards to an abstraction, any renaming of these with the abstraction to another variable name is an idempotent operation, and can be done at any time.
The need for this comes from the unintentional possible binding of a variable during a substitution step. A short example:
\begin{equation}
  \begin{split}
    & \text{Let } F = \lambda xy.yx \text{, then } \forall M, N: \\
    & FMN \equiv NM
  \end{split}
\end{equation}
This would follow from the inductive definition of substitution, however, when taking $M = y$ and $N = x$, this leads to the expression $xy \equiv xx$.
This comes due to the fact that the substitution of $N$ in $M$ should not capture any free variables in $N$.
\subsubsection{Reduction}\label{reduction}
A reduction in $\lambda$-calculus can be defined as a conversion between terms that contracts the term tree. In this sense, it can be seen as a simplification of the multiple abstractions and applications in a term to a more simple, albeit long, form. Once a term is at a point were it has no possible reduction possible, it is in what is called \enquote{normal form}.
It is simple to see the relation with the concept of reduction in TRS, and most of the terminology introduced in that section can be used in regards to $\lambda$.
\paragraph{Reduction} Let $\textbf{R}$ be a notion of reduction on $\Lambda$. Then $\textbf{R}$ induces the binary relations:
\begin{equation}
  \begin{split}
          &{\rightarrow}_{R} \ \textit{one step R-reduction} \\
          &\rightarrow_{R}^{*} \textit{R-reduction} \\
          &=_{R} \ \textit{R-equality or R-convertibility}
  \end{split}
\end{equation}
On this simple idea of reduction we can define the classical notion of reduction in the lambda calculus, $\beta$-reduction. When talking about measuring time complexity in the $lambda$-calculus, this is a good place to start, as it is the main computational device used.
It is based on the substitution rule introduced earlier:
\begin{equation}
  \beta : ( \lambda x.M ) N \rightarrow M [ x:=N ]
\end{equation}
An introduction to some terms associated with reduction; An \textit{R-redex} is a term or subterm that is not in $R$-normal form.
Now when talking about introducing a \enquote{computational cost},$\beta$-reduction is the reasonable choice, as it seems to provide a relation to transitions in a Turing machine. However, the problem in this case is with the arbitrary duplication of terms that can occur during a reduction.
But before diving into that, a brief explanation of reduction strategies.
\paragraph{Reduction Strategies}
When talking about reduction strategies, we refer to a map $F : \Lambda \rightarrow \Lambda$ such that for all terms $M \in \Lambda : M \xrightarrow{*} F(M)$. That is, there exists one and only one term to which $M$ reduces in this map. Strategies are defined as terminating if for every $M$ with a normal form, for some $n \in \mathbb{N}$,$ F^{n} (M)$ is in normal form, where $F^{n}$ represents the n-th step in the reduction path of $M$. \\
The reduction strategy that~\cite{beta-invariance} uses is named Leftmost Outermost. This is a normalizing strategy where if $M$ is not in normal form, the leftmost-outermost redex is reduced until it is. Formally, we can define this in the syntax of contexts inductively, but first we introduce the notion of ordering on terms. As ppreviously stated, TRS have a notion of ordering that is based on strings, but for a better understanding of it, it could also be seen as a tree. The ordering introduced by Accatoli and Dal Lago in their paper is based on the idea of contexts, generalizing the notion and allowing for relative positioning of subterms without needing to understand or define the whole term. It is defined as follows~\cite{beta-invariance}:
\begin{enumerate}
  \item Outside-in order:
        \begin{enumerate}
          \item Root: $\langle \cdot \rangle \prec_{O} C$ for every context $C \neq \langle \cdot \rangle$.
          \item Contextual closure: If $C \prec_{O} D$ then $E\langle C \rangle \prec_{O} E\langle D \rangle $ for any context $E$.
        \end{enumerate}
  \item Left-to-right order:
        \begin{enumerate}
          \item Application: If $C \prec_{p} t$ and $D \prec_{p} u$ then $Cu \prec_{L} tD$.
          \item Contextual Closure: If $C \prec_{L} D$ then  $E\langle C \rangle \prec_{L} E\langle D\rangle$ for any context $E$.
        \end{enumerate}
\end{enumerate}
Both of these orderings are partial orders in relation to terms. A total order is achieved by joining the two:
\begin{center}
  If $C \prec_{O} D$ and $C \prec_{L} D$ then $C \prec_{LO} D$.
\end{center}

As mentioned in the introduction, there is a family of terms in $\lambda$ that in a linear number of steps, reduces to a term of exponential size. Consider the example provided by Accatoli and Dal Lago in~\cite{beta-invariance}. It is introduced here to make the following sections easier to follow. We will also define the ordering of the subterms, and show that $LO$ is a total order on it.
\begin{center}
  Let $u = yxx$, and consider the sequence of terms $t_{n}$  \\ for $n \in \mathbb{N}$ be defined inductively as \\
  $t_{0} = u \ , \ t_{n + 1} = (\lambda x.t_{n})u$ for every $n \in \mathbb{N}$.
\end{center}

\begin{lemma}For any term of the size-exploding family $t_{n}$ there exists a derivation that in $n-1$ $\beta$-reduction steps, reaches a form $s_{n} = (\lambda x. u)r_{n-1}$.
\end{lemma}
  \begin{proof}
  By induction, we first verify the base case, that is, that $t_{1}$ reduces to a term of form $(\lambda x . u) r_{0}$ in $0$ steps, that is, that it already has the form. By the definition of the size exploding family, the term $t_{1} \equiv (\lambda x . t_{0})u$, and since $t_{0} \equiv u$, the induction holds for the base case.


We proceed with the inductive step. If we assume that it holds for $t_{n}$, that is:
\begin{center}
\begin{equation}
  t_{n} \equiv (\lambda x . t_{n-1})u \xrightarrow{n-1}_{\beta} (\lambda x . u)r_{n-1} \equiv s_{n}
\end{equation}
\end{center}
then for $t_{n+1}$ we have:
\begin{center}
\begin{equation}
    t_{n+1} \equiv (\lambda x . t_{n}) u \equiv \underbrace{(\lambda x .(\lambda x . t_{n-1})u)u}_{\beta -r_{0}}
  \end{equation}
  \end{center}
 Where $\beta -r_{0}$ is the redex that we will reduce first, as defined by the LO strategy. We will use this strategy due to the fact that it is normalizing. Since every subsequent $x$ inside $t_{n-1}$ is bound by the previous $\lambda x$, it follows that the only replacements possible are the members of the innermost (explicitly written) $u$, the member of $t_{n}$. A one step reduction produces the term $(\lambda . t_{n-1}) r_{1}$. In one more step, the subterm $r_{1}$ will be duplicated, leading to $r_{2}$, and so on and so forth until we reach $(\lambda x . t_{0})r_{n}$.
\end{proof}
As an example for the rest of the paper, we will use the term $t_{2}$ from this family. In order to define the ordering of this term, it must be first converted to a context based syntax:
$$C_{0} \langle \lambda x. C_{1} \langle \lambda x. C_{3} \langle C_{4} \langle yxx \rangle C_{5} \langle yxx \rangle \rangle C_{2} \langle yxx \rangle \rangle $$
An ordering on this term in $LO$ would then be:
\begin{equation}
 C_{0} \prec_{LO} C_{1} \prec_{LO} C_{2} \prec_{LO} C_{3} \prec_{LO} C_{4} \prec_{LO} C_{5}
\end{equation}
by means of the contextual closure of $LO$.
For further reading on the syntax and axioms of the lambda calculus, refer to~\cite{barendregt1984lambda}.
\subsection{LSC}
In order to properly illustrate the properties of the LSC, we will use as an example the \textbf{size exploding} family that was introduced in \ref{intro-lambda}. Any member of this family could be a good example, but for brevity, this paper will use the previously introduced $t_{3} \equiv \lambda x.((\lambda x. (yxx))(yxx)(yxx))(yxx)$. As explained before, this term, under regular $\beta$-reduction, will evaluate to an exponential length in a number of steps linear to its size, the main obstacle in the invariance of $\lambda$.
The linear substitution calculus is based in \textbf{explicit substitutions}, as oposed to the regular implicit substitutions that $beta$-reduction produces. The grammar is defined as follows:
\begin{equation}
  t, u, r, p = x \ | \ \lambda x.t \ | \ tu \ | \ t[x \leftarrow u]
\end{equation}
where $t[x \leftarrow u]$ is an explicit substitution.
We need to introduce the idea of a new kind of contexts in order to properly define the reduction strategies for the proof of invariance. Besides the addition of explicit substitution to regular contexts, we introduce \textbf{shallow} and \textbf{substitution} contexts ($S$ and $L$ respectively):
\begin{equation}
  \begin{split}
    &S = \langle \cdot \rangle \ | \ \lambda x.S \ | \ St \ | \ tS \ | S[x \leftarrow t] \\
    &L = \langle \cdot \langle \ | L[x \leftarrow t]
  \end{split}
\end{equation}


\section{Proof Overview}
As stated before the measure employed to analyze the time invariance of lambda calculus is the number of transitions in a turing machine. By means of the Linear Substitution Calculus, it is possible to represent even size-exploding terms in Turing machines in polynomial time.
\subsection{High level implementation systems}
The purpose of the high level implementation system definition is to provide a rewriting system invariant to lambda calculus. This step is a bridge of sorts in between lambda calculus and turing machines. For this, we need to define this class of rewriting systems, and which properties should they satisfy in order to be invariant to lambda calculus. We want spcifically termintation and polynomial overhead.
$$\rightsquigarrow \text{terminates iff} {\rightsquigarrow}_{X} \text{terminates}$$
Furthermore,
$$t {\rightsquigarrow}_{X}^k u \text{ iff } t {\rightsquigarrow}^h u\downarrow \text{with } O(h) \in O(k^n) \text{ for some } n \in \mathbb{R} $$
\subsubsection{Properties of high level systems}
This section will provide a basis for the properties of high level systems, and define the properties that they contain.
\subsection{Useful derivations}
The use of
\section{Comparing LSC terms}
- Look into the algorithm to compare them and talk about it
- If we can define an equality relation in the LSC, then we can prove basically the same as we can in Lambda calculus.
\section{Conclusion}
\printbibliography
\end{document}
